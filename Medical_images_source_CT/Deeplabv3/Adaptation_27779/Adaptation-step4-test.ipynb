{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbcca3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import h5py\n",
    "import time\n",
    "import pickle\n",
    "import tempfile\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display as ipy_display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9172c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting path\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b8fc375",
   "metadata": {},
   "outputs": [],
   "source": [
    "deplabv3 = __import__('Deeplabv3')\n",
    "import networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d41b65cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = __import__('dataset-step4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e88a619",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cc3ea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "batch_size = 1\n",
    "suffix = 'run4'\n",
    "epoch = 100\n",
    "epoch = str(epoch)\n",
    "\n",
    "dataset_name = 'abdomen'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219b6862",
   "metadata": {},
   "source": [
    "# CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a664850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109b6635",
   "metadata": {},
   "source": [
    "# Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8189c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_mr_train_dir = \"../../data/h5py/\"\n",
    "source_mr_test_dir = \"../../data/h5py/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "320ed641",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ct_train_dir = \"../../data/h5py/\"\n",
    "target_ct_test_dir = \"../../data/h5py/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61aca8e",
   "metadata": {},
   "source": [
    "# label_ids_abdomen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ad6a7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ids_abdomen = {\"ignore\": 0,\n",
    "    \"lv_myo\": 1,\n",
    "    \"la_blood\": 2,\n",
    "    \"lv_blood\": 3,\n",
    "    \"aa\": 4,\n",
    "}\n",
    "label_ids = label_ids_abdomen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a5836",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "516bd3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(dataset, batch_size=20, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    data_dir = dataset.data_dir\n",
    "    num_samples = len(dataset)\n",
    "    sample_indices = np.random.choice(num_samples, batch_size, replace=False) # replace=True allow repeat\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for idx in sample_indices:\n",
    "        \n",
    "        data_vol, label_vol = dataset[idx]\n",
    "        \n",
    "        images.append(data_vol)\n",
    "        labels.append(label_vol)\n",
    "\n",
    "    images = torch.stack(images)\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5bd51c",
   "metadata": {},
   "source": [
    "#  sliding_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c7e46ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(input_volume, window_size=(32, 32, 32), stride=(16, 16, 16)):\n",
    "    \n",
    "    z_max = input_volume.shape[0] - window_size[0] + 1\n",
    "    x_max = input_volume.shape[1] - window_size[1] + 1\n",
    "    y_max = input_volume.shape[2] - window_size[2] + 1\n",
    "\n",
    "    windows = []\n",
    "\n",
    "    for y in range(0, y_max, stride[2]):\n",
    "        for x in range(0, x_max, stride[1]):\n",
    "            for z in range(0, z_max, stride[0]):\n",
    "                window = input_volume[z:z+window_size[0], x:x+window_size[1], y:y+window_size[2]]\n",
    "                windows.append(window)\n",
    "\n",
    "            # z_remaining\n",
    "            z_remaining = input_volume.shape[0] - window_size[0] # z_remaining = 78 - 32 = 46\n",
    "            window = input_volume[z_remaining:, x:x+window_size[1], y:y+window_size[2]]\n",
    "            windows.append(window)\n",
    "        \n",
    "        # x_remaining\n",
    "        x_remaining = input_volume.shape[1] - window_size[1] # z_remaining = 78 - 32 = 46\n",
    "        for z in range(0, z_max, stride[0]):\n",
    "            window = input_volume[z:z+window_size[0], x_remaining: , y:y+window_size[2]]\n",
    "            windows.append(window)\n",
    "            \n",
    "        # x_remaining z_remaining\n",
    "        z_remaining = input_volume.shape[0] - window_size[0] # z_remaining = 78 - 32 = 46\n",
    "        window = input_volume[z_remaining:, x_remaining: , y:y+window_size[2]]\n",
    "        windows.append(window)\n",
    "    \n",
    "    # y_remaining\n",
    "    y_remaining = input_volume.shape[2] - window_size[2] # z_remaining = 78 - 32 = 46\n",
    "    for x in range(0, x_max, stride[1]):\n",
    "        for z in range(0, z_max, stride[0]):\n",
    "            window = input_volume[z:z+window_size[0], x:x+window_size[1], y_remaining: ]\n",
    "            windows.append(window)\n",
    "            \n",
    "        # y_remaining z_remaining\n",
    "        z_remaining = input_volume.shape[0] - window_size[0] # z_remaining = 78 - 32 = 46\n",
    "        window = input_volume[z_remaining:, x:x+window_size[1], y_remaining:]\n",
    "        windows.append(window)\n",
    "\n",
    "    # y_remaining x_remaining\n",
    "    x_remaining = input_volume.shape[1] - window_size[1] # z_remaining = 78 - 32 = 46\n",
    "    for z in range(0, z_max, stride[0]):\n",
    "        window = input_volume[z:z+window_size[0], x_remaining: , y_remaining:]\n",
    "        windows.append(window)\n",
    "\n",
    "    # y_remaining x_remaining z_remaining\n",
    "    z_remaining = input_volume.shape[0] - window_size[0] # z_remaining = 78 - 32 = 46\n",
    "    window = input_volume[z_remaining:, x_remaining: , y_remaining:]\n",
    "    windows.append(window)\n",
    "\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db64dccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_windows(window_outputs, input_volume_shape, window_size=(32, 32, 32), stride=(16, 16, 16)):\n",
    "    num_classes = window_outputs[0].shape[1] # 5\n",
    "    combined_prob = torch.zeros((num_classes,) + input_volume_shape).to(device)\n",
    "    count_matrix = torch.zeros(input_volume_shape).to(device)\n",
    "\n",
    "    z_max = input_volume_shape[0] - window_size[0] + 1\n",
    "    x_max = input_volume_shape[1] - window_size[1] + 1\n",
    "    y_max = input_volume_shape[2] - window_size[2] + 1\n",
    "\n",
    "    idx = 0\n",
    "    \n",
    "    \n",
    "    for y in range(0, y_max, stride[2]):\n",
    "        for x in range(0, x_max, stride[1]):\n",
    "            for z in range(0, z_max, stride[0]):\n",
    "                output = window_outputs[idx].squeeze() # output.cpu().numpy().shape: (5, 32, 256, 256)\n",
    "                combined_prob[:, z:z+window_size[0], x:x+window_size[1], y:y+window_size[2]] += output\n",
    "                count_matrix[z:z+window_size[0], x:x+window_size[1], y:y+window_size[2]] += 1\n",
    "                idx += 1\n",
    "                \n",
    "\n",
    "            # z_remaining\n",
    "            z_remaining = input_volume_shape[0] - window_size[0] # z_remaining = 78 - 32 = 46\n",
    "            output = window_outputs[idx].squeeze()\n",
    "            combined_prob[:, z_remaining:, x:x+window_size[1], y:y+window_size[2]] += output\n",
    "            count_matrix[z_remaining:, x:x+window_size[1], y:y+window_size[2]] += 1\n",
    "            idx += 1\n",
    "        \n",
    "        # x_remaining\n",
    "        x_remaining = input_volume_shape[1] - window_size[1] # z_remaining = 78 - 32 = 46\n",
    "        for z in range(0, z_max, stride[0]):\n",
    "            output = window_outputs[idx].squeeze()\n",
    "            combined_prob[:, z:z+window_size[0], x_remaining: , y:y+window_size[2]] += output\n",
    "            count_matrix[z:z+window_size[0], x_remaining: , y:y+window_size[2]] += 1\n",
    "            idx += 1\n",
    "            \n",
    "            \n",
    "        # x_remaining z_remaining\n",
    "        z_remaining = input_volume_shape[0] - window_size[0] # z_remaining = 78 - 32 = 46\n",
    "        output = window_outputs[idx].squeeze()\n",
    "        combined_prob[:, z_remaining:, x_remaining: , y:y+window_size[2]] += output\n",
    "        count_matrix[z_remaining:, x_remaining: , y:y+window_size[2]] += 1\n",
    "        idx += 1\n",
    "        \n",
    "    \n",
    "    # y_remaining\n",
    "    y_remaining = input_volume_shape[2] - window_size[2] # z_remaining = 78 - 32 = 46\n",
    "    for x in range(0, x_max, stride[1]):\n",
    "        for z in range(0, z_max, stride[0]):\n",
    "            output = window_outputs[idx].squeeze()\n",
    "            combined_prob[:, z:z+window_size[0], x:x+window_size[1], y_remaining: ] += output\n",
    "            count_matrix[z:z+window_size[0], x:x+window_size[1], y_remaining: ] += 1\n",
    "            idx += 1\n",
    "            \n",
    "            \n",
    "        # y_remaining z_remaining\n",
    "        z_remaining = input_volume_shape[0] - window_size[0] # z_remaining = 78 - 32 = 46\n",
    "        output = window_outputs[idx].squeeze()\n",
    "        combined_prob[:, z_remaining:, x:x+window_size[1], y_remaining:] += output\n",
    "        count_matrix[z_remaining:, x:x+window_size[1], y_remaining:] += 1\n",
    "        idx += 1\n",
    "        \n",
    "\n",
    "    # y_remaining x_remaining\n",
    "    x_remaining = input_volume_shape[1] - window_size[1] # z_remaining = 78 - 32 = 46\n",
    "    for z in range(0, z_max, stride[0]):\n",
    "        output = window_outputs[idx].squeeze()\n",
    "        combined_prob[:, z:z+window_size[0], x_remaining: , y_remaining:] += output\n",
    "        count_matrix[z:z+window_size[0], x_remaining: , y_remaining:] += 1\n",
    "        idx += 1\n",
    "\n",
    "    # y_remaining x_remaining z_remaining\n",
    "    z_remaining = input_volume_shape[0] - window_size[0] # z_remaining = 78 - 32 = 46\n",
    "    output = window_outputs[idx].squeeze()\n",
    "    combined_prob[:, z_remaining:, x_remaining: , y_remaining:] += output\n",
    "    count_matrix[z_remaining:, x_remaining: , y_remaining:] += 1\n",
    "    idx += 1\n",
    "    \n",
    "    \n",
    "    # Normalize the class probabilities\n",
    "    combined_prob /= count_matrix\n",
    "\n",
    "    # Take the argmax of the accumulated probabilities\n",
    "    combined_output = torch.argmax(combined_prob, dim=0)\n",
    "\n",
    "    return combined_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a303273d",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12c6b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = dataset.get_dataloader( target_ct_train_dir,  target_ct_test_dir, num_classes, batch_size,  domain = 'target' )\n",
    "\n",
    "train_dataset = dataloader[\"train\"].dataset\n",
    "#test_dataset = train_dataset\n",
    "test_dataset = dataloader[\"test\"].dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f203058d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dpv3 = deplabv3.DeepLabV3(num_classes)\n",
    "classifier = networks.classifier(num_classes)\n",
    "\n",
    "dpv3 = dpv3.to(device)\n",
    "classifier = classifier.to(device)\n",
    "\n",
    "# parallel\n",
    "# dpv3 = torch.nn.DataParallel(dpv3)\n",
    "# classifier = torch.nn.DataParallel(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac00b49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continue_run\n",
      "Loaded model weights\n"
     ]
    }
   ],
   "source": [
    "dpv3_checkpoint = torch.load('./Adaptation-record-data_212453/' + 'adapted_dpv3_weights_' + str(epoch) + '.pth')\n",
    "classifier_checkpoint = torch.load('./Adaptation-record-data_212453/' + 'adapted_classifier_weights_' + str(epoch) + '.pth')\n",
    "\n",
    "dpv3.load_state_dict(dpv3_checkpoint)\n",
    "classifier.load_state_dict(classifier_checkpoint)\n",
    "print(\"continue_run\")\n",
    "print(\"Loaded model weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed94f4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501\n"
     ]
    }
   ],
   "source": [
    "with open('Adaptation-record-data/' + \"target_test_DICE_history\"  + \".pkl\", \"rb\") as file:\n",
    "    target_test_DICE_history = pickle.load(file)\n",
    "    \n",
    "print(len(target_test_DICE_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb083f37-d845-4cf1-87cf-e36440535ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4775098721874303,\n",
       " 0.49527064772393115,\n",
       " 0.5128423373126403,\n",
       " 0.526445941833545,\n",
       " 0.5361100963189329,\n",
       " 0.5430511060624604,\n",
       " 0.5475633285307606,\n",
       " 0.5518639278134937,\n",
       " 0.5529408476027919,\n",
       " 0.5563122375918402,\n",
       " 0.5600430640962427,\n",
       " 0.5576930067846647,\n",
       " 0.553509920546911,\n",
       " 0.5504216817059737,\n",
       " 0.5526165269714419,\n",
       " 0.5580557500050256,\n",
       " 0.5617812894060246,\n",
       " 0.5630230518586522,\n",
       " 0.5638202020989463,\n",
       " 0.5688086819453508,\n",
       " 0.5690243081016861,\n",
       " 0.5671168156637993,\n",
       " 0.5710253787259716,\n",
       " 0.5717647309209649,\n",
       " 0.5711688531227186,\n",
       " 0.5756462975783782,\n",
       " 0.5776149341222632,\n",
       " 0.5736238149658237,\n",
       " 0.5652002053999178,\n",
       " 0.5636925948355758,\n",
       " 0.5688341097692922,\n",
       " 0.5665411885860673,\n",
       " 0.5680463623599266,\n",
       " 0.5676359046925347,\n",
       " 0.5676116992480286,\n",
       " 0.5706799325800338,\n",
       " 0.5703742582433403,\n",
       " 0.5680132481317876,\n",
       " 0.561471257115268,\n",
       " 0.5635946620676926,\n",
       " 0.563605614750684,\n",
       " 0.5688931228069504,\n",
       " 0.5676490533860836,\n",
       " 0.5687286715789288,\n",
       " 0.5696945659646813,\n",
       " 0.5755392982075787,\n",
       " 0.5651801013532026,\n",
       " 0.5605532011063854,\n",
       " 0.5658929152281031,\n",
       " 0.5698986026463783,\n",
       " 0.5692032961437303,\n",
       " 0.5752850381424641,\n",
       " 0.5768296788223954,\n",
       " 0.5758170702271421,\n",
       " 0.576603653581223,\n",
       " 0.5839894505769432,\n",
       " 0.5798319503733135,\n",
       " 0.5807204962049569,\n",
       " 0.5831808460926908,\n",
       " 0.5801133450843086,\n",
       " 0.576089979187546,\n",
       " 0.5757013930806636,\n",
       " 0.5786051560188302,\n",
       " 0.5753042623430189,\n",
       " 0.5665372113945094,\n",
       " 0.5670796178784195,\n",
       " 0.5700398907273474,\n",
       " 0.5704549525464362,\n",
       " 0.5701287594506022,\n",
       " 0.5704670301881282,\n",
       " 0.5738455243080784,\n",
       " 0.5756842711017671,\n",
       " 0.5754582556272361,\n",
       " 0.5730803021812816,\n",
       " 0.576561149661112,\n",
       " 0.573245290718959,\n",
       " 0.5743367628485607,\n",
       " 0.5665763904575314,\n",
       " 0.5669836198193678,\n",
       " 0.5703666426737519,\n",
       " 0.5649314159597696,\n",
       " 0.5655441237421032,\n",
       " 0.5774007214348654,\n",
       " 0.5704112834543564,\n",
       " 0.573728554097006,\n",
       " 0.5709413196133268,\n",
       " 0.5653849084908236,\n",
       " 0.5669018771085954,\n",
       " 0.565439543217247,\n",
       " 0.5614805824786924,\n",
       " 0.5603987115284701,\n",
       " 0.5625961965474162,\n",
       " 0.5582459990765736,\n",
       " 0.5618545681181518,\n",
       " 0.5538675544567708,\n",
       " 0.5549365333803273,\n",
       " 0.5597837854276162,\n",
       " 0.5620092096468492,\n",
       " 0.561995732884168,\n",
       " 0.5690187899350102,\n",
       " 0.5664010712357367,\n",
       " 0.5666751862624593,\n",
       " 0.5627913518298492,\n",
       " 0.5600127901436526,\n",
       " 0.5631076840398447,\n",
       " 0.5692403065505627,\n",
       " 0.5625261640607773,\n",
       " 0.561410149667694,\n",
       " 0.5575650955632325,\n",
       " 0.5551906411332556,\n",
       " 0.5528700615897608,\n",
       " 0.5560078413087834,\n",
       " 0.5573313808341126,\n",
       " 0.5552795605843426,\n",
       " 0.5552839352343995,\n",
       " 0.5608482446524397,\n",
       " 0.5638261501865373,\n",
       " 0.5611942746886245,\n",
       " 0.5625707055144946,\n",
       " 0.5616792152489102,\n",
       " 0.5581748930270803,\n",
       " 0.5601314245627308,\n",
       " 0.5663979962935048,\n",
       " 0.5699195986416501,\n",
       " 0.5649909761061894,\n",
       " 0.5579681715515404,\n",
       " 0.566506957635833,\n",
       " 0.5648325714750466,\n",
       " 0.5656911810816424,\n",
       " 0.5644378323473399,\n",
       " 0.5609734932821242,\n",
       " 0.5628165244584606,\n",
       " 0.5650437773427044,\n",
       " 0.5669527604694896,\n",
       " 0.569068493554214,\n",
       " 0.5713541869422191,\n",
       " 0.5712168177706126,\n",
       " 0.5712306367208264,\n",
       " 0.5772234361557136,\n",
       " 0.5770513904041159,\n",
       " 0.5699869467446699,\n",
       " 0.5688524185291993,\n",
       " 0.5669162802445429,\n",
       " 0.5695646346921112,\n",
       " 0.5699724773053073,\n",
       " 0.5685506144585044,\n",
       " 0.5682235221401064,\n",
       " 0.5702368731564676,\n",
       " 0.5726992320339642,\n",
       " 0.572375644459725,\n",
       " 0.5638078509142684,\n",
       " 0.5647472389662285,\n",
       " 0.56527545696033,\n",
       " 0.5684996274827089,\n",
       " 0.5680450337786305,\n",
       " 0.5739279473600483,\n",
       " 0.5768833664437665,\n",
       " 0.5774225100683429,\n",
       " 0.5785986079583947,\n",
       " 0.5761865828211903,\n",
       " 0.5774502658464532,\n",
       " 0.5810113918887585,\n",
       " 0.5794732776814937,\n",
       " 0.5829487970477798,\n",
       " 0.5849489630605301,\n",
       " 0.5886722107335359,\n",
       " 0.5841571726575742,\n",
       " 0.581934146332832,\n",
       " 0.5829915418372037,\n",
       " 0.5831264123165731,\n",
       " 0.5796757693408385,\n",
       " 0.5816386624388985,\n",
       " 0.5858050472271958,\n",
       " 0.5797900540084735,\n",
       " 0.5817218276444234,\n",
       " 0.5778479796959753,\n",
       " 0.5801288150663649,\n",
       " 0.582073227072927,\n",
       " 0.583704587931902,\n",
       " 0.5886045376990663,\n",
       " 0.5838587968107778,\n",
       " 0.5852379730487851,\n",
       " 0.5887286069720802,\n",
       " 0.5853599048239855,\n",
       " 0.5749608868849684,\n",
       " 0.5785403910768072,\n",
       " 0.5759877885914557,\n",
       " 0.5749080887740327,\n",
       " 0.5701904224299073,\n",
       " 0.5708567829850535,\n",
       " 0.5703779539305859,\n",
       " 0.5731018551017613,\n",
       " 0.5740067932211839,\n",
       " 0.574589706786434,\n",
       " 0.5707736926030716,\n",
       " 0.5628695598819898,\n",
       " 0.5620865720219752,\n",
       " 0.5641816025806543,\n",
       " 0.5658798537243485,\n",
       " 0.5680515840351283,\n",
       " 0.5688071295761717,\n",
       " 0.5716670207423495,\n",
       " 0.570798774629337,\n",
       " 0.5631793899239966,\n",
       " 0.5681954540206636,\n",
       " 0.568864734528872,\n",
       " 0.571502879942745,\n",
       " 0.5733966361349393,\n",
       " 0.5762138343666606,\n",
       " 0.5745886240685267,\n",
       " 0.5728195474175716,\n",
       " 0.5747533761990824,\n",
       " 0.5775840094464352,\n",
       " 0.5781022019030482,\n",
       " 0.5737725263032705,\n",
       " 0.5725315491739176,\n",
       " 0.5715365683050537,\n",
       " 0.5734016050382237,\n",
       " 0.575440623134328,\n",
       " 0.5737262646663233,\n",
       " 0.5751215296013027,\n",
       " 0.5771329781229951,\n",
       " 0.5766352612995584,\n",
       " 0.573487496930287,\n",
       " 0.5792974325629623,\n",
       " 0.5838591614886058,\n",
       " 0.5780821684833506,\n",
       " 0.5831129469105633,\n",
       " 0.5838580005268591,\n",
       " 0.5821381523439437,\n",
       " 0.5786430992453766,\n",
       " 0.5806590272409183,\n",
       " 0.5785953878686784,\n",
       " 0.5724707159603686,\n",
       " 0.5684285740346742,\n",
       " 0.5637855813899258,\n",
       " 0.5601275360924768,\n",
       " 0.5636598285608244,\n",
       " 0.564180339738581,\n",
       " 0.5564741522850628,\n",
       " 0.559307543658945,\n",
       " 0.5653400637684529,\n",
       " 0.5616151317453061,\n",
       " 0.5596397922418761,\n",
       " 0.5618891252022835,\n",
       " 0.556392954224516,\n",
       " 0.5628120230816441,\n",
       " 0.5667764603830588,\n",
       " 0.5701203015278677,\n",
       " 0.5652161096348434,\n",
       " 0.5571894268594405,\n",
       " 0.5606779440871209,\n",
       " 0.5654059740816073,\n",
       " 0.5706852181892863,\n",
       " 0.5684605774031136,\n",
       " 0.5648641557323273,\n",
       " 0.5612663721765461,\n",
       " 0.5662352950123453,\n",
       " 0.5609229753139264,\n",
       " 0.5642511154383029,\n",
       " 0.5722786969363747,\n",
       " 0.572090483516934,\n",
       " 0.572984763868736,\n",
       " 0.5712247728064084,\n",
       " 0.5705311254625414,\n",
       " 0.5741033707086126,\n",
       " 0.5752804410598156,\n",
       " 0.580312187053562,\n",
       " 0.571154207637265,\n",
       " 0.5717902977546806,\n",
       " 0.5645583970569263,\n",
       " 0.5587324327957423,\n",
       " 0.5587417865849271,\n",
       " 0.560511343856578,\n",
       " 0.5578669893085326,\n",
       " 0.5556716053345633,\n",
       " 0.5626428166032393,\n",
       " 0.5669505803349673,\n",
       " 0.5663818258375692,\n",
       " 0.5687948406638681,\n",
       " 0.5686461318115386,\n",
       " 0.5683075688812175,\n",
       " 0.5627497935861593,\n",
       " 0.5691650641132134,\n",
       " 0.5624349987289041,\n",
       " 0.557813678489014,\n",
       " 0.5622867151396114,\n",
       " 0.5663123382386689,\n",
       " 0.5657241750797318,\n",
       " 0.5610142836337682,\n",
       " 0.5600217241067589,\n",
       " 0.5600179096498535,\n",
       " 0.5581536484850327,\n",
       " 0.5577612322786819,\n",
       " 0.5607479991380669,\n",
       " 0.5606411021300499,\n",
       " 0.5593603237150956,\n",
       " 0.5624403208348565,\n",
       " 0.5587649924599964,\n",
       " 0.5619356456917588,\n",
       " 0.5690358917969314,\n",
       " 0.574711154421999,\n",
       " 0.5747237542106013,\n",
       " 0.5688296665791769,\n",
       " 0.5735371052703157,\n",
       " 0.56475828839972,\n",
       " 0.566784662352872,\n",
       " 0.5602877210178098,\n",
       " 0.5579900145891669,\n",
       " 0.5611145142152436,\n",
       " 0.5598610311149137,\n",
       " 0.5628808209012434,\n",
       " 0.5657310981325024,\n",
       " 0.5691251589705317,\n",
       " 0.564964821154661,\n",
       " 0.5676304280041872,\n",
       " 0.5656986773876364,\n",
       " 0.5653899667818378,\n",
       " 0.5664094444597464,\n",
       " 0.5708143755837684,\n",
       " 0.5676079125872571,\n",
       " 0.5685404934075252,\n",
       " 0.5712700407397575,\n",
       " 0.5679653723551504,\n",
       " 0.572316710598143,\n",
       " 0.5712403585342495,\n",
       " 0.5708146268561133,\n",
       " 0.565310037438641,\n",
       " 0.5665011461042376,\n",
       " 0.5627983103572956,\n",
       " 0.5619940975140003,\n",
       " 0.5682354170149959,\n",
       " 0.5731837609114908,\n",
       " 0.5727099475228044,\n",
       " 0.5778127650580045,\n",
       " 0.5820814959046037,\n",
       " 0.5787277058037843,\n",
       " 0.5815099150360917,\n",
       " 0.578736313461072,\n",
       " 0.580868386519316,\n",
       " 0.5737959346080306,\n",
       " 0.5726996036902685,\n",
       " 0.5708273066985605,\n",
       " 0.5779301204052633,\n",
       " 0.5758931503222113,\n",
       " 0.5703155126539707,\n",
       " 0.5731304386203651,\n",
       " 0.5754384709860686,\n",
       " 0.5704873190141901,\n",
       " 0.5719883181754407,\n",
       " 0.5745809035527044,\n",
       " 0.5794403619646226,\n",
       " 0.5728969845836014,\n",
       " 0.5763010583533231,\n",
       " 0.5747057599420022,\n",
       " 0.5742309831451261,\n",
       " 0.5777982489799937,\n",
       " 0.5810123337351265,\n",
       " 0.5835827073805278,\n",
       " 0.5773567469389244,\n",
       " 0.5736454872178911,\n",
       " 0.5736316531414799,\n",
       " 0.5824211839898263,\n",
       " 0.5814854482720491,\n",
       " 0.5802934025827222,\n",
       " 0.5810478072264655,\n",
       " 0.580023272923997,\n",
       " 0.5796993838574939,\n",
       " 0.5813838440185591,\n",
       " 0.5840247049654149,\n",
       " 0.5859729629582158,\n",
       " 0.5801893738162834,\n",
       " 0.5799652133443267,\n",
       " 0.582162080836085,\n",
       " 0.5828841879693525,\n",
       " 0.5877291140711076,\n",
       " 0.5816735581581023,\n",
       " 0.574324699527325,\n",
       " 0.5726403623245453,\n",
       " 0.5679403721883434,\n",
       " 0.5605732838377266,\n",
       " 0.557116437014106,\n",
       " 0.5569204187833866,\n",
       " 0.5549582086523368,\n",
       " 0.5605421516431165,\n",
       " 0.5633155330128992,\n",
       " 0.5686742610999824,\n",
       " 0.5621637656007317,\n",
       " 0.5592424672933769,\n",
       " 0.5620790695339515,\n",
       " 0.5573103263469819,\n",
       " 0.5564757066688976,\n",
       " 0.5572553309330184,\n",
       " 0.5581291473407116,\n",
       " 0.5647681946549029,\n",
       " 0.5690351870280353,\n",
       " 0.5702063973583272,\n",
       " 0.5699561352586481,\n",
       " 0.5693117879567948,\n",
       " 0.5639291656943684,\n",
       " 0.5648887476579465,\n",
       " 0.5658162248779938,\n",
       " 0.5640832495050997,\n",
       " 0.5576493132846748,\n",
       " 0.5622638399013972,\n",
       " 0.557574155081126,\n",
       " 0.5569799735256389,\n",
       " 0.5579203748666001,\n",
       " 0.5598067551298187,\n",
       " 0.5610130633185827,\n",
       " 0.5668057541328975,\n",
       " 0.5669743065173036,\n",
       " 0.5670354910083822,\n",
       " 0.5679101001496907,\n",
       " 0.5682615848371873,\n",
       " 0.5649524787571316,\n",
       " 0.5666589481670158,\n",
       " 0.5661898159782707,\n",
       " 0.5679286785180564,\n",
       " 0.5714150080271184,\n",
       " 0.5735155676744561,\n",
       " 0.574163334734809,\n",
       " 0.5805424889892845,\n",
       " 0.5765297859189946,\n",
       " 0.5730237524606911,\n",
       " 0.5694043892902039,\n",
       " 0.5681746301014011,\n",
       " 0.57128286129844,\n",
       " 0.5719161971115531,\n",
       " 0.5716550551348643,\n",
       " 0.5736021533907684,\n",
       " 0.5704948169867852,\n",
       " 0.567266466162237,\n",
       " 0.5764629849010487,\n",
       " 0.5755751365349047,\n",
       " 0.569774401375476,\n",
       " 0.5711973937127719,\n",
       " 0.5743464121572942,\n",
       " 0.5715312339962816,\n",
       " 0.5755281581023882,\n",
       " 0.5714070789313818,\n",
       " 0.5746238541798097,\n",
       " 0.5635340844156808,\n",
       " 0.5609860074193361,\n",
       " 0.5668757448237776,\n",
       " 0.5697915947591792,\n",
       " 0.5679206319554801,\n",
       " 0.565938590626085,\n",
       " 0.5629969596698063,\n",
       " 0.5609961201285313,\n",
       " 0.5591454553641524,\n",
       " 0.5588877141091713,\n",
       " 0.553980549458148,\n",
       " 0.5487895904335568,\n",
       " 0.550234974444952,\n",
       " 0.5594265527373243,\n",
       " 0.5510464467656458,\n",
       " 0.5546522347419689,\n",
       " 0.5568328088880754,\n",
       " 0.555953908343476,\n",
       " 0.5545894031605639,\n",
       " 0.5528774190245407,\n",
       " 0.5553507667711063,\n",
       " 0.5535802848579959,\n",
       " 0.5541604723330977,\n",
       " 0.5530294747719612,\n",
       " 0.5585463707348152,\n",
       " 0.5566456642122496,\n",
       " 0.5602890387337549,\n",
       " 0.5585907641785471,\n",
       " 0.5619963865599079,\n",
       " 0.5663079022533137,\n",
       " 0.5705676147843579,\n",
       " 0.5674832140427827,\n",
       " 0.5680030589163663,\n",
       " 0.5685725329750133,\n",
       " 0.560856193095956,\n",
       " 0.5680159599514399,\n",
       " 0.5647446971829986,\n",
       " 0.5691162297495925,\n",
       " 0.5662509733970537,\n",
       " 0.5688950409956293,\n",
       " 0.5699856053233829,\n",
       " 0.568325519405533,\n",
       " 0.5698948953013943,\n",
       " 0.5677220299548046,\n",
       " 0.5683506315463897,\n",
       " 0.5699282719612844,\n",
       " 0.5705097334909404,\n",
       " 0.5661256506375659,\n",
       " 0.5617842470596774,\n",
       " 0.5584621260878629,\n",
       " 0.5564679771858119,\n",
       " 0.5546536585376949,\n",
       " 0.5585876062089914,\n",
       " 0.5562093991164454,\n",
       " 0.5578097048463309,\n",
       " 0.5580380727029491,\n",
       " 0.5568843736095729,\n",
       " 0.5563042754872373,\n",
       " 0.5537715834642176]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test_DICE_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df57b02",
   "metadata": {},
   "source": [
    "# Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "107b7856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rs37890/.conda/envs/diffusion/lib/python3.9/site-packages/torch/nn/modules/conv.py:605: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1702400441250/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv3d(\n"
     ]
    }
   ],
   "source": [
    "test_output = []\n",
    "\n",
    "for img_idx in range(len(test_dataset)): # 0, 1, 2, 3\n",
    "    \n",
    "    data_vol, label_vol = test_dataset[img_idx] # data_vol: torch.Size([1, 60, 512, 512])\n",
    "    data_vol = data_vol.to(device)\n",
    "    label_vol = label_vol.to(device)\n",
    "    \n",
    "    data_vol = torch.squeeze(data_vol, 0) # data_vol:  torch.Size([60, 512, 512])\n",
    "    windows = sliding_window(data_vol) # slice 3D image based on window size and stride\n",
    "    \n",
    "    \n",
    "    \n",
    "    window_outputs = []\n",
    "    \n",
    "    dpv3.eval()\n",
    "    classifier.eval() \n",
    "    with torch.no_grad():\n",
    "        for window in windows:\n",
    "            window = window.unsqueeze(0)  # Add a channel dimension: torch.Size([1, 32, 256, 256])\n",
    "            window = torch.unsqueeze(window, 0)  # Add a batch dimension: torch.Size([1, 1, 32, 256, 256])\n",
    "            \n",
    "            # inference\n",
    "            output = dpv3(window)\n",
    "            output = classifier(output) # torch.Size([1, 5, 32, 256, 256])\n",
    "            \n",
    "            # collect outputs\n",
    "            window_outputs.append(output)  # len(window_outputs) = 27\n",
    "            # window_outputs[0].cpu().numpy().shape： (1, 5, 32, 256, 256)\n",
    "\n",
    "    combined_output = combine_windows(window_outputs, data_vol.size())\n",
    "    test_output.append(combined_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1e4c86",
   "metadata": {},
   "source": [
    "# evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bd7c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_arrays = [tensor.cpu().numpy() for tensor in test_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b621ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136, 146, 126)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_arrays[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0666c662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 112, 80)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_arrays[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3539c6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149, 113, 123)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_arrays[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ba4431d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127, 171, 75)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_arrays[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88c35487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127, 171, 75)\n",
      "(127, 171, 75)\n"
     ]
    }
   ],
   "source": [
    "id_to_ignore = 0\n",
    "intersection = dict()\n",
    "total = dict()\n",
    "for label in label_ids:\n",
    "    intersection[label] = total[label] = 0\n",
    "\n",
    "\n",
    "\n",
    "for img_idx in range(len(test_dataset)): # 0, 1, 2, 3\n",
    "\n",
    "    if img_idx != 3:\n",
    "        continue\n",
    "    \n",
    "    _, y_true = test_dataset[img_idx] # data_vol: torch.Size([1, 60, 512, 512])\n",
    "    \n",
    "    y_hat = numpy_arrays[img_idx]\n",
    "    y_true = y_true.cpu().numpy() \n",
    "    \n",
    "    print(y_hat.shape)\n",
    "    print(y_true.shape)\n",
    "    \n",
    "    for label in label_ids:\n",
    "        if label_ids[label] == id_to_ignore:\n",
    "            continue\n",
    "\n",
    "        curr_id = label_ids[label]\n",
    "\n",
    "        idx_gt = y_true == curr_id\n",
    "        idx_hat = y_hat == curr_id\n",
    "\n",
    "        intersection[label] += 2 * np.sum(idx_gt & idx_hat)\n",
    "        total[label] += np.sum(idx_gt) + np.sum(idx_hat)\n",
    "        \n",
    "        \n",
    "dice = []\n",
    "res = dict()\n",
    "for label in label_ids:\n",
    "    if label_ids[label] == id_to_ignore:\n",
    "        continue\n",
    "\n",
    "    if total[label] != 0:\n",
    "        res[label] = intersection[label] / total[label]\n",
    "    else:\n",
    "        print('total is zero')\n",
    "        res[label] = np.float64(0)\n",
    "\n",
    "    dice.append(res[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "619ea79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5488314309285357"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b15c658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lv_myo 0.6454941527172667\n",
      "la_blood 0.36313486568817505\n",
      "lv_blood 0.6578027587801603\n",
      "aa 0.5288939465285408\n"
     ]
    }
   ],
   "source": [
    "for k in res:\n",
    "    print(k, res[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2836db96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe84c400-2ba2-4957-9ecc-8ca098745e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b75a3c71-76b4-41d0-a1aa-6f61b412d0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127, 171, 75)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4e4759-299d-461d-898c-81b8ab3f183c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9de72faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_slice = int(y_true.shape[2] / 2)\n",
    "\n",
    "\n",
    "# #nlplt.plot_epi(img)\n",
    "# plt.imshow(y_true[:, :, z_slice], cmap='jet')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6933711a-cff8-4370-8235-f5d400d8ab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_slice = int(y_hat.shape[2] / 2)\n",
    "# slice_img = y_hat[:, :, z_slice]\n",
    "\n",
    "# # Check if the slice_img is empty or not\n",
    "# if slice_img.size == 0:\n",
    "#     print(\"The image slice is empty. Check your slicing indices.\")\n",
    "# else:\n",
    "#     print(f\"Shape: {slice_img.shape}, Data Type: {slice_img.dtype}\")\n",
    "\n",
    "#     # Convert to a suitable type if necessary\n",
    "#     if slice_img.dtype != np.uint8:\n",
    "#         slice_img = (255 * (slice_img - slice_img.min()) / (slice_img.max() - slice_img.min())).astype(np.uint8)\n",
    "\n",
    "#     # Define the new size you want for the square image\n",
    "#     square_size = 256\n",
    "\n",
    "#     # Resize the image to square\n",
    "#     square_img = cv2.resize(slice_img, (square_size, square_size), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "#     # Plot the image\n",
    "#     plt.imshow(square_img, cmap='jet')\n",
    "#     plt.axis('off')  # Turn off axis numbers and ticks\n",
    "#     plt.show()\n",
    "\n",
    "#     plt.imsave('Post-Adapt_3.png', square_img, cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c5f15e-5b39-40ed-beb9-7bd227c2839e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
